"""
helper function for importing
clean and tidy tables, but also manage files scaning
"""

import hashlib
import os
import re
import subprocess
import sys
from argparse import Namespace
from datetime import date

import pandas as pd
from pandas.core.dtypes.cast import NAType

from app import sql
from app.common import (
    check_dir_file,
    first_diff_index,
    foreign_tabs,
    get_alternatives,
    match_from_list,
    read_json_dict,
    store_alternatives,
    tab_cols,
    unpack_foreign,
    vimdiff_config,
)
from app.error import (
    AmbigousMatchError,
    CheckDirError,
    NoMatchError,
    PrepareTabError,
    ScanDirPermissionError,
    SqlTabError,
    VimdiffSelError,
)
from app.message import MessageHandler
from conf.config import *  # pylint: disable=unused-wildcard-import,wildcard-import

msg = MessageHandler()


def import_tab(dat: pd.DataFrame, tab: str, args: Namespace, file: str) -> None:
    """prepare tab as per config and sql_scheme and import"""
    # rename (and tidy) columns according to format of imported file
    # apply configuration from config.py
    dat = columns_align(
        dat.copy(),
        file=file,
        args=args,
    )

    # existing device for summary info reasons
    ex_devs = sql.getL(tab="DEVICE", get_col=[DEV_HASH])

    try:
        # align table with sql definition
        # remove NAs in mandatory columns
        dat = prepare_tab(
            dat=dat.copy(),
            tab=tab,
            file=file,
            row_shift=import_format[args.format]["header"],
        )
    except PrepareTabError as e:
        msg.msg(str(e))
        return

    # if we have stored alternatives, use it
    dat.loc[:, DEV_MAN], _ = get_alternatives(dat[DEV_MAN].to_list())

    # hash columns - must be last so all columns aligned and present
    dat = hash_tab(dat=dat)

    # align other cols before skipping data in incoming data
    on_conflict = None
    if not args.dont_align_columns:
        on_conflict = {"action": "REPLACE"}
        existing_data = sql.getDF(
            tab="DEVICE",
            search=dat.loc[:, DEV_HASH],
            where=[DEV_HASH],
        )
        try:
            miss_cols = existing_data.columns.difference(dat.columns)
            dat[miss_cols] = pd.DataFrame(
                None, index=dat.index, columns=miss_cols, dtype=object
            )
            existing_data["dev_rm"] = existing_data[DEV_ID]
            existing_data["man_rm"] = existing_data[DEV_MAN]
            dat = align_other_cols(rm_dat=existing_data, keep_dat=dat)
        except ValueError as e:
            # wrong input for align_other_cols()
            print(e)
        except KeyError:
            # no existing data
            pass
        except KeyboardInterrupt as e:
            print(e)
            sys.exit(1)
    # check if data already in sql
    if tab == "BOM" and not check_existing_project(dat, args):
        return  # user do not want to overwrite nor add to existing data
    if tab == "STOCK" and not check_existing_data(dat, args):
        return

    # check for different manufacturer on the same dev_id
    # just inform that alignment can be done with admin functions
    align_data(dat=dat, just_inform=True)

    # inform if data useful for other tabs is present
    tabs = tabs_in_data(dat)
    if "SHOP" in tabs and tab != "SHOP":
        msg.msg("Detected data usefull also for SHOP table.")
        msg.msg("Consider importing with 'shop_cart_import' option.")
    # write aligned data to SQL
    for t in ["DEVICE", tab]:
        if t == "DEVICE":
            sql.put(dat=dat, tab=t, on_conflict=on_conflict)
        else:
            sql.put(dat=dat, tab=t)

    # SUMMARY
    if tab == "BOM":
        msg.bom_import_summary(dat, len(dat[dat[BOM_HASH].isin(ex_devs)]))


def tabs_in_data(dat: pd.DataFrame) -> list[str]:
    """
    return all tbles which mandatary cols are present in dat
    """
    sql_scheme = read_json_dict(SQL_SCHEME)
    # need to start from DEVICE because other tables refer to it
    tabs = sorted(list(sql_scheme.keys()), key=lambda x: (x != "DEVICE", x))
    for t in tabs[:]:
        must_cols, _ = tab_cols(t)
        # check if all required columns are in aligned_dat
        missing_cols = [c for c in must_cols if c not in dat.columns]
        if any(missing_cols):
            tabs.remove(t)
    return tabs


def prepare_tab(
    dat: pd.DataFrame,
    tab: str,
    file: str,
    row_shift: int,
) -> pd.DataFrame:
    """
    prepares and check if data aligned with table.
    iterate through tabs and check if mandatory columns present
    return only tables with all mandatory columns
    and sanitazed data

    check columns: mandatary, nice to have
    """
    must_cols, nice_cols = tab_cols(tab)

    # check if all required columns are in new_stock
    missing_cols = [c for c in must_cols if c not in dat.columns]
    # if 'project' column is missing, take file name col, inform user
    if BOM_PROJECT in missing_cols:
        dat[BOM_PROJECT] = dat[BOM_FILE].apply(lambda cell: cell.split(".")[0])
        missing_cols = [c for c in missing_cols if c != BOM_PROJECT]
        msg.project_as_filename()
    # must after 'project' column creation, otherway possibly missing
    if any(missing_cols):
        raise PrepareTabError(tab, file, missing_cols)

    # clean text, leave only ASCII
    # i.e. easyEDM writes manufactuere in Chinese in parenthesis
    for c in dat.columns:
        if dat[c].dtype == "object":
            dat[c] = dat[c].apply(ASCII_txt)

    # remove rows with NA in must_cols
    # must be afterASCII_txt, becouse it is possible to have only chinese in must_col
    dat = NA_rows(dat, must_cols, nice_cols, row_shift)

    # and strip text
    for col in dat.select_dtypes(include=["object"]).columns:
        dat[col] = dat[col].apply(lambda x: x.strip() if isinstance(x, str) else x)

    return dat


def hash_tab(dat: pd.DataFrame) -> pd.DataFrame:
    """
    hash columns as per foreign key in SQLscheme
    also unpack foreign key to make sure all columns present
    """
    sql_scheme = read_json_dict(SQL_SCHEME)

    def apply_hash(row: pd.Series, cols: list[str]) -> str:
        combined = "".join(str(row[c]) for c in cols)
        return hashlib.sha256(combined.encode()).hexdigest()

    def hash_t(t):
        hash_cols = sql_scheme[t].get("HASH_COLS", [])
        if hash_cols:
            dat["hash"] = dat.apply(lambda row: apply_hash(row, hash_cols), axis=1)

    tabs = tabs_in_data(dat)
    for t in tabs[:]:
        tabs += foreign_tabs(t)
    tabs = list(set(tabs))

    for t in tabs:
        hash_t(t)

    # add foreign col in case it's not present yet
    # for example if we have FOREIGN:[{'dev_hash':'dev(hash)'}]
    # col hash exists, but now we need to copy hash to dev_hash
    for t in tabs:
        for f in sql_scheme[t].get("FOREIGN", []):
            to_col, _, from_col = unpack_foreign(f)
            dat[to_col] = dat[from_col]
    return dat


def NA_rows(  # pylint: disable=invalid-name
    df: pd.DataFrame,
    must_cols: list[str],
    nice_cols: list[str],
    row_shift: int = 0,
    inform: bool = True,
) -> pd.DataFrame:
    """
    check rows with any NA

    inform user and remove from data,
    remove only when NA in must rows
    """
    row_shift += 2  # one for header, and one to start from zero
    na_rows = df.loc[df.loc[:, must_cols].isna().any(axis=1)]
    na_rows_id: list[int] = [int(c) + row_shift for c in na_rows.index.values]
    df = df.loc[~df.index.isin(na_rows.index)]

    # check for nice cols
    nicer_cols = [c for c in nice_cols if c in df.columns]
    na_rows = df.loc[df.loc[:, nicer_cols].isna().any(axis=1)]
    if inform:
        msg.na_rows(rows=na_rows, row_id=na_rows_id)
    return df


def columns_align(n_stock: pd.DataFrame, file: str, args: Namespace) -> pd.DataFrame:
    """basic columns align per config.py"""
    # lower columns
    n_stock.rename(
        columns={c: str(c).lower() for c in n_stock.columns},
        inplace=True,
    )
    # drop columns if any col in values so to avoid duplication
    if cols := import_format[args.format].get("cols"):
        n_stock.drop(
            [v for _, v in cols.items() if v in n_stock.columns],
            axis="columns",
            inplace=True,
        )
        # then rename
        n_stock.rename(
            columns=cols,
            inplace=True,
        )

    # apply formatter functions
    if f := import_format[args.format].get("func"):
        n_stock = n_stock.apply(f, axis=1, result_type="broadcast")  # type: ignore

    # change columns type
    # only for existing cols
    n_stock = n_stock.astype(object)
    if dtype := import_format[args.format].get("dtype"):
        exist_col_dtypes = {k: v for k, v in dtype.items() if k in n_stock.columns}
        try:
            n_stock = n_stock.astype(exist_col_dtypes)
        except ValueError as e:
            msg.msg(e.__str__())
            sys.exit(1)

    # Strip whitespace from all string elements in the DataFrame
    n_stock = n_stock.map(lambda x: x.strip() if isinstance(x, str) else x)

    # add column with path and file name and supplier
    n_stock[BOM_DIR] = os.path.abspath(args.dir)
    n_stock[BOM_FILE] = os.path.basename(file)
    n_stock[BOM_FORMAT] = args.format
    n_stock[SHOP_DATE] = date.today().strftime("%Y-%m-%d")
    if SHOP_SHOP not in n_stock.columns:
        n_stock[SHOP_SHOP] = import_format[args.format].get("shop", args.format)

    return n_stock


def ASCII_txt(  # pylint: disable=invalid-name
    txt: str | None | NAType,
) -> str | None | NAType:  # pylint: disable=invalid-name
    """remove any chinese signs from string columns"""
    if not isinstance(txt, str):
        return txt
    if pd.isna(txt):
        return None
    txt = str(txt).replace("Î©", "ohm")
    txt = txt.encode("ascii", "ignore").decode("ascii")
    # remove any empty paranthases '()' from string columns
    txt = re.sub(r"\(.*?\)", "", txt)
    if not txt:  # only chinese letters
        return None
    return txt


def align_data(dat: pd.DataFrame, just_inform: bool = False) -> pd.DataFrame:
    """
    align new data with existing:
        1. align manufacturers
        2. where manufacturer changed, align all other columns
        3. remove devs where manufacturer changed
        4. return aligned data to be added
    if just_inform==True, just collect duplication and display, no action
    return dataframe with changed devices with aditional column 'dev_rm'
    with device hashes before change
    raise KeyboardInterrupt when user abort
    """
    align_dat = align_manufacturers(
        dat.copy(deep=True),
        just_inform=just_inform,
    )
    if just_inform:
        return dat
    # select rows where change
    differ_row = dat[DEV_MAN] != align_dat[DEV_MAN]
    # create new hash after possibly changing manufacturer
    align_dat = hash_tab(dat=align_dat).loc[differ_row, :]
    # if all differ_row is False, no change (data aligned or align manufacture did the job)
    if align_dat.empty:
        msg.msg("Data is aligned. Nothing done.")
        return align_dat
    # keep new device in separate dataframe
    keep_dev_hash = align_dat[DEV_HASH].drop_duplicates().to_list()
    keep_dev = dat[dat[DEV_HASH].isin(keep_dev_hash)]
    if len(keep_dev) != len(keep_dev_hash):
        # new manufacturer name somwhere, device not existing in db
        new_dev_hash = [c for c in keep_dev_hash if c not in dat[DEV_HASH]]
        keep_dev = pd.concat(
            [
                keep_dev,
                align_dat[align_dat[DEV_HASH].isin(new_dev_hash)].drop_duplicates(
                    subset=[DEV_HASH]
                ),
            ],
        )
    # possibly empty if no differ rows
    if not any(differ_row.to_list()):
        return dat
    # mark devs that will be removed (dev_rm and man_rm)
    align_dat = pd.concat(
        [
            dat.loc[differ_row, [DEV_HASH, DEV_MAN]].rename(
                columns={
                    DEV_HASH: "dev_rm",
                    DEV_MAN: "man_rm",
                }
            ),  # fmt: ignore
            align_dat,
        ],
        axis="columns",
    )  # fmt: ignore
    # align all other dev cols
    keep_dev = align_other_cols(rm_dat=align_dat, keep_dat=keep_dev)  # pyright: ignore
    # add data from other tabs for devs we are going to remove
    # don't take old hashes, but rehash again
    keep_dev = pd.merge(
        left=keep_dev,
        right=align_dat.loc[:, ["dev_rm", DEV_HASH]],
        on=DEV_HASH,
        how="right",
    )
    keep_dev = sql.getDF_other_tabs(
        dat=keep_dev,
        hash_list=keep_dev["dev_rm"].to_list(),
        merge_on="dev_rm",
    )
    keep_dev = hash_tab(keep_dev)
    return keep_dev


def align_other_cols(  # pylint: disable=too-many-locals
    rm_dat: pd.DataFrame,
    keep_dat: pd.DataFrame,
) -> pd.DataFrame:
    """
    expect DataFrame with one pair replacement per row:
    and columns:
        rm_dat: dev_rm | man_rm | DEV_HASH | DEV_MAN + must_cols
        keep_dat: must_cols+nice_cols
    display all "nice" attributes of device to user to choose
    dispalay only not empty and attributes that differ
    if keep_attr empty, replace with rm_attr
    return selected attributes attached in row to input dat
    raise KeyboardInterrupt when user abort
    """
    if rm_dat.empty or keep_dat.empty:
        raise ValueError("Input DataFrame can not be empty.")

    extra_cols = ["dev_rm", "man_rm"]
    must_cols, nice_cols = tab_cols(tab="DEVICE")

    missing_rm_cols = [c for c in must_cols + extra_cols if c not in rm_dat.columns]
    if any(missing_rm_cols):
        raise ValueError(f"Missing necessery columns in rm_dat: {missing_rm_cols}")
    missing_keep_cols = [c for c in must_cols + nice_cols if c not in keep_dat.columns]
    if any(missing_keep_cols):
        raise ValueError(f"Missing necessery columns in keep_dat: {missing_keep_cols}")

    keep_dat.set_index(DEV_HASH, inplace=True)
    # collect all useful data from devices we are about to remove
    for idx in rm_dat.index:
        rm_attr = rm_dat.copy(deep=True).loc[idx, :]
        # collect attributes for devices that we want to use
        keep_attr = keep_dat.loc[keep_dat.index == rm_attr[DEV_HASH]].iloc[0, :]
        change_man = keep_attr[DEV_MAN]
        opt_man = rm_attr["man_rm"]
        ref_id = rm_attr[DEV_ID]
        rm_attr, keep_attr, keep_nones = align_attributes(rm_attr, keep_attr)
        if not keep_attr.empty or not keep_nones.empty:
            try:
                aligned_dat = vimdiff_selection(
                    ref_col={"columns": rm_attr.index.to_list()},
                    change_col={change_man: keep_attr.to_list()},
                    opt_col={opt_man: rm_attr.to_list()},
                    what_differ="attributes",
                    dev_id=ref_id,
                    exit_on_change=False,
                )
            except VimdiffSelError as err:
                # if no change from user, skip
                print(str(err))
                if err.interact:
                    # raised when user mess up with lines order or number
                    input("Press any key....")
                continue
            keep_attr.iloc[:] = aligned_dat
            # in case keep_attr is missing indexes
            keep_attr = keep_attr.reindex(keep_attr.index.union(keep_nones.index))
            keep_attr.update(keep_nones)
            # update will put NaN in missing cols, which will brake on int columns
            # so update on common cols only
            keep_attr_df = pd.DataFrame([keep_attr])
            common_col = keep_dat.columns.intersection(keep_attr_df.columns)
            keep_dat.loc[keep_attr_df.index, common_col] = keep_attr_df
    return keep_dat.reset_index()


def align_attributes(rm_attr: pd.Series, keep_attr: pd.Series) -> tuple:
    """
    Align attributes of devices:
        rm_attr: this attrbutes will lost if not aligned
        keep_attr: this we optionaly may want to keep
    """
    # if NA in rm_attr or missing col, take from keep_attr if present
    for idx, val in keep_attr.items():
        if idx not in rm_attr:
            rm_attr[idx] = val
        if rm_attr[idx] is None:
            rm_attr[idx] = keep_attr[idx]
    # and oposite, then will be easy to remove the same
    for idx, val in rm_attr.items():
        if idx not in keep_attr:
            keep_attr[idx] = val
    # hide attributes which are already aligned
    for idx in rm_attr.index:
        rm, keep = rm_attr[idx], keep_attr[idx]
        # IEEE754: NaN is not equal to anything, inluding itself!
        if (rm == keep) or (pd.isna(rm) & pd.isna(keep)):
            rm_attr.pop(idx)
            keep_attr.pop(idx)
    # if None in keep_dat and not in rm_dat, use and write separate Series
    index_none = rm_attr.loc[keep_attr.isna() & rm_attr.notna()].index
    keep_none = rm_attr[index_none]
    rm_attr.drop(index_none, inplace=True, errors="ignore")
    keep_attr.drop(index_none, inplace=True, errors="ignore")
    return (
        rm_attr.sort_index(),
        keep_attr.sort_index(),
        keep_none,
    )


def align_manufacturers(  # pylint: disable=too-many-return-statements
    dat: pd.DataFrame,
    just_inform: bool = False,
) -> pd.DataFrame:
    """
    For device_id duplication, collect manufacturer name if different
    create dictionary with list of manufacturers as values for device_id as key:
    {DEV_ID:[MAN1,MAN2,...]}
    1.  checking only incoming data against stock. Do not check duplication
        inside incoming data
    2.  simplest case is one-to-one or many-to-one: only one device in stock
    3.  tricky is when are many devices in stock: present manufacturers separated with
        '|' as list. Special macro in vim to pick option
    return DataFrame modified by user
    when 'just_inform', just display messages about possible duplications
    and ref to admin funcs
    """
    start_line = 1  # start line for vim (vim count from 1)
    if not all(c in dat.columns for c in [DEV_ID, DEV_MAN]):
        return dat
    ex_dat = sql.getDF(
        tab="DEVICE",
        get_col=[DEV_ID, DEV_MAN],
        search=dat[DEV_ID].to_list(),
        where=[DEV_ID],
    )
    if ex_dat.empty:
        # no duplications in stock
        return dat

    # group existing data on dev, join manufacturers with ' | ' if more then one dev
    ex_grp = (
        ex_dat.groupby(DEV_ID)[DEV_MAN]
        .agg(lambda x: " | ".join(x.astype(str)))
        .reset_index()
    )

    # iterate as long as there is a change from user
    while True:
        # panda RULES: must be merged on 'left' to keep indexes, and clean up NAs later
        # with merge on 'inner' indexed are fuck up
        dat_dup = pd.merge(
            left=dat,
            right=ex_grp,
            on=DEV_ID,
            how="left",
            suffixes=("", "_opts"),
        )

        # column name for grouped manufacturers
        man_grp_col = DEV_MAN + "_opts"

        # for GREAT panda NaN is any value so must be droped before
        dat_dup = dat_dup.loc[~dat_dup[man_grp_col].isna(), :]

        # drop rows with matched manufacturer
        dat_dup = dat_dup[dat_dup[DEV_MAN] != dat_dup[man_grp_col]]

        if dat_dup.empty:
            # importing manufacturers are aligned with stock
            return dat

        # remove dup_col from dup_col_grp
        for r in dat_dup.itertuples():
            dup_col_grp_v = getattr(r, man_grp_col)
            dup_col_v = getattr(r, DEV_MAN)
            opts = str(dup_col_grp_v).split(" | ")
            opts = [o for o in opts if o != dup_col_v]
            dat_dup.loc[r.Index, man_grp_col] = " | ".join(opts)

        if just_inform:
            msg.inform_duplications(dup=dat_dup.loc[:, [DEV_MAN, man_grp_col, DEV_ID]])
            return dat
        # sort data on device_id, much easier to understand in vimdiff
        dat_dup.sort_values(by=DEV_ID, inplace=True)
        try:
            chosen = vimdiff_selection(
                ref_col={"devices": dat_dup.loc[:, DEV_ID].to_list()},
                change_col={DEV_MAN: dat_dup.loc[:, DEV_MAN].to_list()},
                opt_col={man_grp_col: dat_dup.loc[:, man_grp_col].to_list()},
                what_differ=DEV_MAN,
                dev_id="",
                exit_on_change=True,
                start_line=start_line,
            )
        except KeyboardInterrupt:
            msg.msg("Interupted by user. Changes discarded.")
            return pd.DataFrame()
        except VimdiffSelError as err:
            # user messed up with line numbers
            print(str(err))
            return pd.DataFrame()
        # if no change from user, finish
        if dat_dup[DEV_MAN].to_list() == chosen:
            break
        # find index of change, so can start vim on correct line number
        start_line = first_diff_index(dat_dup[DEV_MAN].to_list(), chosen)
        dat_dup[DEV_MAN] = chosen
        # put new data into orginal tab, based on preserved indexes
        dat.loc[dat_dup.index, DEV_MAN] = dat_dup[DEV_MAN]

    return dat


def vimdiff_selection(  # pylint: disable=too-many-positional-arguments,too-many-locals,too-many-arguments
    ref_col: dict[str, list[str]],
    change_col: dict[str, list[str]],
    opt_col: dict[str, list[str]],
    what_differ: str,
    dev_id: str,
    exit_on_change: bool,
    start_line: int = 1,
) -> list[str]:
    """
    present alternatives in vimdiff
    input is dict, wher key is column_name and values are to be displayed
    column_name is also used to name the file where values are written and then displayed by vimdiff
    """
    cols = []
    for col in [ref_col, change_col, opt_col]:
        cols.append(next(iter(col)))
        cols.append(next(iter(col.values())))
    ref_k, ref_v = (0, 1)
    change_k, change_v = (2, 3)
    opt_k, opt_v = (4, 5)
    # if empty list (nothing to write, nothing to show)
    # just return
    if cols[change_v] == [] or cols[opt_v] == []:
        return []

    for key in [ref_k, change_k, opt_k]:
        # remove forbiden chars: / \ : * ? " < > |
        cols[key] = re.sub(r'[\/\\:\*\?"<>\|\r\n\t]', "_", cols[key])
        with open(
            # in case manufacturers are the same, need to add suffix to file name
            TEMP_DIR + cols[key] + "_" + str(key) + ".txt",
            mode="w",
            encoding="UTF8",
        ) as f:
            for ind, item in enumerate(cols[key + 1]):
                f.write(str(ind) + "| " + str(item) + "\n")  # add line number: '1| txt'
                # other way diff may shift rows to align data between columns

    vimdiff_config(
        ref_col=cols[ref_k] + "_0",
        change_col=cols[change_k] + "_2",
        opt_col=cols[opt_k] + "_4",
        what_differ=what_differ,
        dev_id=dev_id,
        exit_on_change=exit_on_change,
        start_line=start_line,
    )

    vim_cmd = "vim -u " + TEMP_DIR + ".vimrc"
    if DEBUG == "none":
        subprocess.run(vim_cmd, shell=True, check=False)
    elif DEBUG == "debugpy":
        with subprocess.Popen("konsole -e " + vim_cmd, shell=True) as p:
            p.wait()

    with open(TEMP_DIR + cols[change_k] + "_2.txt", mode="r", encoding="UTF8") as f:
        chosen = f.read().splitlines()

    # clean up files
    for key in [ref_k, change_k, opt_k]:
        os.remove(TEMP_DIR + cols[key] + "_" + str(key) + ".txt")

    if chosen == []:  # user interrupt
        raise KeyboardInterrupt("Interupted by user. Changes discarded.")
    # remove line numbers
    chosen = [re.sub(r"^\d+\|\s*", "", c) for c in chosen]
    chosen = [c.strip() for c in chosen]
    if any(len(chosen) != len(cols[v]) for v in [ref_v, change_v, opt_v]):
        # if user mess-up or added/removed rows
        max_len = max(len(chosen), len(cols[opt_v]))
        chosen += [None] * (max_len - len(chosen))
        cols[opt_v] += [None] * (max_len - len(cols[opt_v]))
        df = pd.DataFrame({"selected": chosen, "opts": cols[opt_v]})
        raise VimdiffSelError(select=df, interact=DEV_MAN != cols[change_k])
    # write matches selected by user, so next time save some time
    if DEV_MAN == cols[change_k]:
        store_alternatives(
            alternatives={
                cols[change_k]: cols[change_v],
                cols[opt_k]: cols[opt_v],
            },
            selection=chosen,
        )
    return chosen


def check_existing_project(dat: pd.DataFrame, args: Namespace) -> bool:
    """
    check if project already present in BOM
    if -overwrite, remove existing data
    other way ask for confirmation
    return True if we can continue
    """
    old_project = sql.getL(tab="BOM", get_col=[BOM_PROJECT])
    project = dat.loc[0, BOM_PROJECT]
    if args.overwrite:
        # remove all old data
        sql.rm(
            tab="BOM",
            value=dat[BOM_PROJECT].to_list(),
            column=[BOM_PROJECT],
        )
        return True
    # warn about adding qty
    if project in old_project:
        if not msg.project_already_imported(project):
            return False
    return True


def check_existing_data(dat: pd.DataFrame, args: Namespace) -> bool:
    """
    check if data already present in STOCK
    if -overwrite, remove existing data
    other way ask for confirmation
    return True if we can continue
    """
    old_data = sql.getL(tab="STOCK", get_col=[STOCK_HASH])
    overlap_data = dat.loc[dat[STOCK_HASH].isin(old_data), :]
    if overlap_data.empty:
        # no existing data so -overwrite dosent make sense
        args.overwrite = False
        return True
    if args.overwrite:
        # remove all old data
        sql.rm(
            tab="STOCK",
            value=overlap_data[STOCK_HASH].to_list(),
            column=[STOCK_HASH],
        )
        return True
    # warn about adding qty
    if not overlap_data.empty:
        if not msg.data_already_imported(overlap_data):
            return False
    return True


def prepare_project(projects: list[str]) -> list[str]:
    """
    prepare list of projects based on provided args:
    - '%' all projects
    - '?' just list projects
    can abreviate names
    """
    all_projects = sql.getL(
        tab="BOM",
        get_col=[BOM_PROJECT],
    )
    if not all_projects:
        msg.bom_prepare_projects([], [])
        return []
    if projects == ["?"]:
        msg.bom_prepare_projects(
            project=[],
            all_projects=all_projects,
        )
        return []
    if projects == ["%"]:
        projects = all_projects
    match_projects = []
    for project in projects:
        try:
            match_projects += [match_from_list(project, all_projects)]
        except AmbigousMatchError as err:
            print(err)
        except NoMatchError as err:
            print(err)

    if match_projects != projects:
        msg.bom_prepare_projects(
            project=match_projects,
            all_projects=[],
        )
    return match_projects


def scan_files(args) -> list[str]:
    """scan for files to be imported for BOM"""
    if "reimport" in args:
        reimport = args.reimport
    else:
        reimport = False
    if not reimport:
        try:
            files = check_dir_file(args)
        except CheckDirError as err:
            msg.msg(str(err))
            sys.exit(1)
        except ScanDirPermissionError as err:
            msg.msg(str(err))
            sys.exit(1)
        if not files:
            msg.import_missing_file()
            sys.exit(1)

    else:
        locations = sql.getDF(
            tab="BOM",
            get_col=[BOM_DIR, BOM_FILE, BOM_PROJECT, BOM_FORMAT],
        )
        if locations.empty:
            msg.reimport_missing_file()
            sys.exit(1)
        files = []
        for _, r in locations.iterrows():
            args.dir = r[BOM_DIR]
            args.file = r[BOM_FILE]
            args.format = r[BOM_FORMAT]
            args.project = r[BOM_PROJECT]
            try:
                f = check_dir_file(args)
            except CheckDirError as e:
                msg.msg(str(e))
                continue
            files += f
        if not files:
            msg.reimport_missing_file()
            sys.exit(1)
        args.overwrite = True
    return list(set(files))


def tab_info(tab: str, silent: bool = False) -> list[str]:
    """diplay info about columns in BOM table"""
    try:
        must_col, nice_col = tab_cols(tab, all_cols=True)
    except SqlTabError as err:
        msg.msg(str(err))
        sys.exit(1)
    must_col.sort()
    nice_col.sort()
    if not silent:
        msg.bom_info(must_col, nice_col, col_description())
    return must_col + nice_col


def tab_template(tab: str, args: Namespace) -> None:
    """save csv tempalete to a file"""
    cols = pd.Series(tab_info(tab=tab, silent=True))
    csv = pd.DataFrame(columns=cols)
    csv.to_csv(args.csv_template, index=False)
    msg.msg(f"template written to {args.csv_template}")


def col_description() -> dict:
    """extract columns descriptions from sql_scheme"""
    sql_scheme = read_json_dict(SQL_SCHEME)
    col_desc = {}
    for _, cont in sql_scheme.items():
        if "COL_DESCRIPTION" in cont:
            for k, v in cont["COL_DESCRIPTION"].items():
                if k not in col_desc:
                    col_desc[k] = v
    return col_desc
